---
layout: section
title: Pipeline Best Practices
---
ifdef::backend-html5[]
:notitle:
:description:
:author: Alex Taylor
:email: jenkinsci-docs@googlegroups.com
:sectanchors:
ifdef::env-github[:imagesdir: ../resources]
ifndef::env-github[:imagesdir: ../../resources]
:hide-uri-scheme:
:toc:
endif::[]

Pipeline Best Practices:

Q: How do I scale pipelines so that they are not using too many resources on the master?

. **Where Does groovy execute?:** First concept that needs to be discussed is that Groovy code *always* executes on master resources. This means that any time groovy methods are being used, the code will be executed by the master and use master memory/CPU. Because of this, the most critical action item is to reduce the number of groovy calls that need to be made (this includes how many imports of groovy methods being used). The most common example methods are things like:
.. JsonSlurper: This would be to read in some file which is on disk into a JSON object within the pipeline so it would look something like this: `def JsonFile = new JsonSlurper().parseText(readFile("$LOCAL_FILE")))`. The issue here is that this loads that file into memory 2x on the master and if that file is very large or this command is being done very often then it will drain a lot of unnecessary memory
... Solution: Instead of using JsonSlurper, use a shell step and return the standard out. This shell would look something like this: `def JsonReturn = sh label: '', returnStdout: true, script: 'echo "$LOCAL_FILE"| jq "."'`. This will use agent resources to read the file and even parse the file with a good `jq` filter
.. HttpRequest: Many times this would be used to grab data from an external source and store it into a variable. This is not a good idea to use because not only is that request coming from the master(which could give incorrect results for things like HTTPS requests if the master does not have certificates loaded) but also the return is again stored twice.
... Solution: Again using the shell step but this time with a `curl` command is the best direction to go.Then have the request come from master and also the data size can be shrunk with filtering.
. **Pipeline functions as Glue:** Make sure to use Pipeline steps as glue rather than Main functionality. This means that rather than relying on pipeline functionality(groovy or pipeline steps) to drive the build process forward, it is better to use single steps(such as `sh`) to accomplish multiple parts of the build. The less pipeline functions used, the less memory will consumed up on master
.. Example: Using a single maven build step to drive the build through its build/test/deploy process.
. **Just say no to Custom Pipeline Steps:** Wherever possible stay away from Customized/overwritten Pipeline Steps. This is the process where the standard pipeline APIs like `sh` or `timeout` are overwritten using shared libraries
.. This is dangerous because the pipeline APIs can change at any time causing custom code to break or give different results than expected
.. It can become difficult to troubleshoot when something is breaking or causing issues
.. Because of the ubiquitous use of these steps throughout pipelines, if something is coded incorrectly/inefficiently the results can be catastrophic to Jenkins
. **Reduce Repetition of Pipeline Steps:** Combine pipeline steps into single steps as often as possible to reduce the size of the flow-node graph. 
.. For each pipeline step that is executed there is a flow-node graph that allows a pipeline to know what step it is currently executing and when resuming, the flow node graph has to be loaded onto the stack
.. This means whenever doing a series of `echo` or `sh` steps make sure to combine them into a single step or script rather than individual steps
. **Dealing with Concurrency within pipelines:** Try not to share workspaces across multiple pipeline executions or multiple distinct pipelines. 
.. This can lead to either unexpected file modification within each pipeline or each workspace will get renamed unexpectedly
.. Ideally shared volumes/disks are mounted somewhere else and the files are copied from one place to the current workspace. Then when the build is done it can be copied back
.. If that is not going to work then make sure that either concurrency is disabled on the pipeline or use the [lockable resources plugin](https://plugins.jenkins.io/lockable-resources) to lock up the workspace when running on it and nothing else can use while it is locked.
... Be aware that both of these slow down the time to results of builds though
. **Using @NonCPS:** If none of these work for a given build process then ensure that functions which are in use have the @NonCPS flag
.. This flag will ensure that a given function is not serialized(the process we use to ensure pipelines are resumable) which will reduce the overall footprint for the function
... Be aware though that no pipeline steps can be in a @NonCPS function because they are automatically serialized regardless of the flag and that will cause the whole function flag to be ignored
. **Patterns to avoid:**
.. Calling `Jenkins.getInstance`
... This will allow pipelines/scripts to have system level(admin) access to any item in the instance and be able to control anything they want data-wise
... Additionally if this is an “approved” method in script security, it will give anyone who can create a job and build it `admin` level access
... This is also additional API calls on the master which can lead to performance issues
.. Large global variable declaration files
... This will load every variable for every pipeline and depending on concurrency can end up using large amounts of memory
.. Large shared libraries
... This will load the same shared library per-job that is currently executing which can lead to memory overhead
... This can also cause a slow down of every pipeline since it will need to checkout a very large file every time before starting

