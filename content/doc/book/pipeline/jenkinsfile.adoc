---
layout: section
title: Using a Jenkinsfile
---
:description:
:author:
:email: jenkinsci-docs@googlegroups.com
:sectanchors:
:toc:
:toclevels: 4
:hide-uri-scheme:

= Using a Jenkinsfile

This section builds on the information covered in <<getting-started#, Getting Started>>,
and introduces more useful steps, common patterns, and demonstrates some
non-trivial `Jenkinsfile` examples.

Creating a `Jenkinsfile`, which is checked into source control
footnoteref:[scm, https://en.wikipedia.org/wiki/Source_control_management],
provides a number of immediate benefits:

* Code review/iteration on the Pipeline
* Audit trail for the Pipeline
* Single source of truth
  footnote:[https://en.wikipedia.org/wiki/Single_Source_of_Truth]
  for the Pipeline, which can be viewed and edited by multiple members of the project.

Pipeline supports <<syntax#, two syntaxes>>, declarative (introduced in
Pipeline 2.5) and scripted Pipeline. Both of which support building continuous
delivery pipelines. Both may be used to define a Pipeline in either the web UI
or with a `Jenkinsfile`, though it's generally considered a best practice to
create a `Jenkinsfile` and check the file into the source control repository.


== Creating a Jenkinsfile

As discussed in the <<getting-started#defining-a-pipeline-in-scm, Getting
Started>> section, a `Jenkinsfile` is a text file that contains the definition
of a Jenkins Pipeline and is checked into source control. Consider the following
Pipeline which implements a basic three-stage continuous delivery pipeline.

[pipeline]
----
// Declarative //
pipeline {
    agent any

    stages {
        stage('Build') {
            steps {
                echo 'Building..'
            }
        }
        stage('Test') {
            steps {
                echo 'Testing..'
            }
        }
        stage('Deploy') {
            steps {
                echo 'Deploying....'
            }
        }
    }
}
// Script //
node {
    stage('Build') {
        echo 'Building....'
    }
    stage('Test') {
        echo 'Building....'
    }
    stage('Deploy') {
        echo 'Deploying....'
    }
}
----

Not all Pipelines will have these same three stages, but it is a good starting
point to define them for most projects. The sections below will demonstrate the
creation and execution of a simple Pipeline in a test installation of Jenkins.

[NOTE]
====
It is assumed that there is already a source control repository set up for
the project and a Pipeline has been defined in Jenkins following
<<getting-started#defining-a-pipeline-in-scm, these instructions>>.
====

Using a text editor, ideally one which supports
link:http://groovy-lang.org[Groovy]
syntax highlighting, create a new `Jenkinsfile` in the root directory of the
project.

[role=declarative-pipeline]
The declarative Pipeline example above contains the minimum necessary structure
to implement a continuous delivery pipeline. The <<syntax#agent, agent
directive>>, which is required, instructs Jenkins to allocate an executor and
workspace for the Pipeline. Without an `agent` directive, not only is the
declarative Pipeline not valid, it would not be capable of doing any work! By
default the `agent` directive ensures that the source repository is checked out
and made available for steps in the subsequent stages`

The <<syntax#stages, stages directive>>, and <<syntax#steps, steps directives>>
are also required for a valid declarative Pipeline as they instruct Jenkins
what to execute and in which stage it should be executed.

[role=scripted-pipeline]
====
For more advanced usage with scripted Pipeline, the example above `node` is
a crucial first step as it allocates an executor and workspace for the Pipeline.
In essence, without `node`, a Pipeline cannot do any work! From within `node`,
the first order of business will be to checkout the source code for this
project.  Since the `Jenkinsfile` is being pulled directly from source control,
Pipeline provides a quick and easy way to access the right revision of the
source code

[pipeline]
----
// Script //
node {
    checkout scm // <1>
    /* .. snip .. */
}
// Declarative not yet implemented //
----
<1> The `checkout` step will checkout code from source control; `scm` is a
special variable which instructs the `checkout` step to clone the specific
revision which triggered this Pipeline run.
====


=== Build

For many projects the beginning of "work" in the Pipeline would be the "build"
stage. Typically this stage of the Pipeline will be where source code is
assembled, compiled, or packaged. The `Jenkinsfile` is *not* a replacement for an
existing build tool such as GNU/Make, Maven, Gradle, etc, but rather can be
viewed as a glue layer to bind the multiple phases of a project's development
lifecycle (build, test, deploy, etc) together.

Jenkins has a number of plugins for invoking practically any build tool in
general use, but this example will simply invoke `make` from a shell step
(`sh`).  The `sh` step assumes the system is Unix/Linux-based, for
Windows-based systems the `bat` could be used instead.

[pipeline]
----
// Declarative //
pipeline {
    agent any

    stages {
        stage('Build') {
            steps {
                sh 'make' // <1>
                archiveArtifacts artifacts: '**/target/*.jar', fingerprint: true // <2>
            }
        }
    }
}
// Script //
node {
    stage('Build') {
        sh 'make' // <1>
        archiveArtifacts artifacts: '**/target/*.jar', fingerprint: true // <2>
    }
}
----
<1> The `sh` step invokes the `make` command and will only continue if a
zero exit code is returned by the command. Any non-zero exit code will fail the
Pipeline.
<2> `archiveArtifacts` captures the files built matching the include pattern
(`**/target/*.jar`) and saves them to the Jenkins master for later retrieval.


[TIP]
====
Archiving artifacts is not a substitute for using external artifact
repositories such as Artifactory or Nexus and should be considered only for
basic reporting and file archival.
====


=== Test

Running automated tests is a crucial component of any successful continuous
delivery process. As such, Jenkins has a number of test recording, reporting,
and visualization facilities provided by a
link:https://plugins.jenkins.io/?labels=report[number of plugins].
At a fundamental level, when there are test failures, it is useful to have
Jenkins record the failures for reporting and visualization in the web UI.  The
example below uses the `junit` step, provided by the
plugin:junit[JUnit plugin].

In the example below, if tests fail, the Pipeline is marked "unstable", as
denoted by a yellow ball in the web UI. Based on the recorded test reports,
Jenkins can also provide historical trend analysis and visualization.

[pipeline]
----
// Declarative //
pipeline {
    agent any

    stages {
        stage('Test') {
            steps {
                /* `make check` returns non-zero on test failures,
                * using `true` to allow the Pipeline to continue nonetheless
                */
                sh 'make check || true' // <1>
                junit '**/target/*.xml' // <2>
            }
        }
    }
}
// Script //
node {
    /* .. snip .. */
    stage('Test') {
        /* `make check` returns non-zero on test failures,
         * using `true` to allow the Pipeline to continue nonetheless
         */
        sh 'make check || true' // <1>
        junit '**/target/*.xml' // <2>
    }
    /* .. snip .. */
}
----
<1> Using an inline shell conditional (`sh 'make || true'`) ensures that the
`sh` step always sees a zero exit code, giving the `junit` step the opportunity
to capture and process the test reports. Alternative approaches to this are
covered in more detail in the <<failure-handling>> section below.
<2> `junit` captures and associates the JUnit XML files matching the inclusion
pattern (`**/target/*.xml`).


=== Deploy

Deployment can imply a variety of steps, depending on the project or
organization requirements, and may be anything from publishing built artifacts
to an Artifactory server, to pushing code to a production system.

At this stage of the example Pipeline, both the "Build" and "Test" stages have
successfully executed. In essense, the "Deploy" stage will only execute
assuming previous stages completed successfully, otherwise the Pipeline would
have exited early.

[pipeline]
----
// Declarative //
pipeline {
    agent any

    stages {
        stage('Deploy') {
            when {
              expression {
                currentBuild.result == null || currentBuild.result == 'SUCCESS' // <1>
              }
            }
            steps {
                sh 'make publish'
            }
        }
    }
}
// Script //
node {
    /* .. snip .. */
    stage('Deploy') {
        if (currentBuild.result == null || currentBuild.result == 'SUCCESS') { // <1>
            sh 'make publish'
        }
    }
    /* .. snip .. */
}
----
<1> Accessing the `currentBuild.result` variable allows the Pipeline to
determine if there were any test failures. In which case, the value would be
`UNSTABLE`.

Assuming everything has executed successfully in the example Jenkins Pipeline,
each successful Pipeline run will have associated build artifacts archived,
test results reported upon and the full console output all in Jenkins.

[role=scripted-pipeline]
A scripted Pipeline can include conditional tests (shown above), loops,
try/catch/finally blocks and even functions. The next section will cover this
advanced scripted Pipeline syntax in more detail.


== Advanced syntax for Pipeline


=== String interpolation

Jenkins Pipeline uses rules identical to
link:http://groovy-lang.org[Groovy]
for string interpolation. Groovy's String interpolation support can be
confusing to many newcomers to the language. While Groovy supports
declaring a string with either single quotes, or double quotes, for
example:

[source,groovy]
----
def singlyQuoted = 'Hello'
def doublyQuoted = "World"
----

Only the latter string will support the dollar-sign (`$`) based string
interpolation, for example:

[source,groovy]
----
def username = 'Jenkins'
echo 'Hello Mr. ${username}'
echo "I said, Hello Mr. ${username}"
----

Would result in:

[source]
----
Hello Mr. ${username}
I said, Hello Mr. Jenkins
----

Understanding how to use string interpolation is vital for using some of
Pipeline's more advanced features.


=== Using environment variables

Jenkins Pipeline exposes environment variables via the global variable `env`,
which is available from anywhere within a `Jenkinsfile`. The full list of
environment variables accessible from within Jenkins Pipeline is documented at
link:http://localhost:8080/pipeline-syntax/globals#env[localhost:8080/pipeline-syntax/globals#env],
assuming a Jenkins master is running on `localhost:8080`, and includes:

BUILD_ID:: The current build ID, identical to BUILD_NUMBER for builds created in Jenkins versions 1.597+
JOB_NAME:: Name of the project of this build, such as "foo" or "foo/bar".
JENKINS_URL:: Full URL of Jenkins, such as http://example.com:port/jenkins/ (NOTE: only available if Jenkins URL set in "System Configuration")

Referencing or using these environment variables can be accomplished like
accessing any key in a Groovy
link:http://groovy-lang.org/syntax.html#_maps[Map],
for example:

[pipeline]
----
// Declarative //
pipeline {
    agent any
    stages {
        stage('Example') {
            steps {
                echo "Running ${env.BUILD_ID} on ${env.JENKINS_URL}"
            }
        }
    }
}
// Script //
node {
    echo "Running ${env.BUILD_ID} on ${env.JENKINS_URL}"
}
----


==== Setting environment variables

Setting an environment variable within a Jenkins Pipeline is accomplished
differently depending on whether declarative or scripted Pipeline is used.

Declarative Pipeline supports an <<syntax#environment, environment>>
directive, whereas users of scripted Pipeline must use the `withEnv` step.

[pipeline]
----
// Declarative //
pipeline {
    agent any
    environment { // <1>
        CC = 'clang'
    }
    stages {
        stage('Example') {
            environment { // <2>
                DEBUG_FLAGS = '-g'
            }
            steps {
                sh 'printenv'
            }
        }
    }
}
// Script //
node {
    /* .. snip .. */
    withEnv(["PATH+MAVEN=${tool 'M3'}/bin"]) {
        sh 'mvn -B verify'
    }
}
----
<1> An `environment` directive used in the top-level `pipeline` block will
apply to all steps within the Pipeline.
<2> An `environment` directive defined within a `stage` will only apply the
given environment variables to steps within the `stage`.


=== Handling credentials

Jenkins can store credentials that may be used:

* anywhere applicable throughout Jenkins (i.e. global credentials),
* by a specific Pipeline project/item,
* by a specific Jenkins user (as is the case for
  link:/doc/book/blueocean/creating-pipelines[Pipeline projects created in Blue
  Ocean]).

These credentials could take the form of:

* a token such as an API token (e.g. a GitHub personal access token), also known
  as "secret text",
* a username and password, which could be handled separately or as a colon
  separated string in the format `username:password`,
* an SSH public/private key pair,
* a "secret file", which is essentially secret content in a file,
* a PKCS#12 certificate file and password, also known as a "certificate", or
* Docker host certificate authentication credentials.

There are numerous 3rd-party sites and applications that can interact with
Jenkins, for example, artifact repositories, cloud-based storage systems and
services, and so on. A systems administrator of such an application can
configure credentials in the application for dedicated use by Jenkins. This is
done typically to "lock down" areas of the application's functionality available
to Jenkins, usually by applying access controls to these credentials. Once a
Jenkins manager (i.e. a Jenkins user who administers a Jenkins site)
adds/configures these credentials in Jenkins, the credentials can be used by
Pipeline projects to interact with these 3rd party applications.

To maximize security, credentials configured in Jenkins are only handled in
Pipeline projects via their credential IDs. This minimizes the chances of
exposing the actual credentials themselves to Jenkins users. When a Jenkins
manager manually adds credentials to Jenkins, the opportunity is provided to
specify a meaningful credential ID value - for example, +
`jenkins-user-for-xyz-artifact-repository`. If this optional field is not
specified, Jenkins assigns a globally unique ID (GUID) value for the credential
ID. Bear in mind that once a credential ID is set, it can no longer be changed.


==== Setting credentials in a Pipeline

Once the Jenkins environment has the necessary credentials configured, these can
easily be specified within your Pipeline for immediate use.


===== Using declative Pipeline syntax

Jenkins' decalartive Pipeline syntax has the `credentials()` helper method (used
within the <<syntax#environment,`environment`>> directive) which supports secret
text, as well as username and password credentials.

For example, the snippet below shows how you can create a Pipeline that uses
environment variables for two "secret text" credentials to access Amazon Web
Services (AWS); one of these credentials has been configured in Jenkins with the
credential ID +
`jenkins-aws-secret-key-id` and the other with `jenkins-aws-secret-access-key`.

[source,groovy]
----
pipeline {
    agent {
        // define agent details
    }
    environment {
        AWS_ACCESS_KEY_ID     = credentials('jenkins-aws-secret-key-id')
        AWS_SECRET_ACCESS_KEY = credentials('jenkins-aws-secret-access-key')
    }
    stages {
        stage('Example stage') {
            steps {
                // Use these credential environment variables, which in this
                // Pipeline example are scoped for the entire Pipeline, by
                // referencing these variables in your steps with the syntax
                // $AWS_ACCESS_KEY_ID and $AWS_SECRET_ACCESS_KEY.
                // For security reasons, if you attempt to retrieve the value of
                // these credential variables from within the Pipeline (e.g.
                // echo $AWS_SECRET_ACCESS_KEY), Jenkins will only return the
                // value "****" to prevent secret information being written to
                // any logs.
            }
        }
    }
}
----

In this example, the credentials assigned to the two `AWS_...` environment
variables are scoped globally for the entire Pipeline. However, like any other
environment variables in a Pipeline, credential variables can be scoped to a
specific stage by moving the `environment` directive containing these variables
into a <<syntax#steps,`steps`>> section.

The `credentials()` method also supports the creation of environment variables
for "username and password" credentials which, in the following example, shows
the creation of credential variables to access a Bitbucket repository:

[source,groovy]
----
environment {
    COMMON_BITBUCKET_REPO_CREDS = credentials('jenkins-common-bitbucket-repo-creds')
}
----

where the credential ID is `jenkins-common-bitbucket-repo-creds`.

This will actually set the following three environment variables:

* `COMMON_BITBUCKET_REPO_CREDS` - contains a username and a password separated
  by a colon in the format `username:password`.
* `COMMON_BITBUCKET_REPO_CREDS_USR` contains the username component only.
* `COMMON_BITBUCKET_REPO_CREDS_PSW` contains the password component only.

[NOTE]
====
By convention, variable names for environment variables are typically specified
in capital case, with individual words separated by underscores. However,
variable names do not need to follow this convention. You can specify any
legitimate variable name using lower case characters. Bear in mind, however,
that the additional environment variables created by the `credentials()` method
(above) will always be appended with `_USR` and `_PSW` (i.e. in the format of an
underscore followed by three capital letters).
====

The following Pipeline example shows how these credentials can be used within
the scope of a single stage:

[source,groovy]
----
pipeline {
    agent {
        // define agent details
    }
    stages {
        stage('Example stage 1') {
            environment {
                COMMON_BITBUCKET_REPO_CREDS = credentials('jenkins-common-bitbucket-repo-creds')
            }
            steps {
                // The following credential environment variables are available
                // for use in this stage block:
                // * $COMMON_BITBUCKET_REPO_CREDS
                // * $COMMON_BITBUCKET_REPO_CREDS_USR
                // * $COMMON_BITBUCKET_REPO_CREDS_PSW
            }
        }
        stage('Example stage 2') {
            steps {
                // However, these credential variables are not available in this
                // stage block.
            }
        }
    }
}
----


===== Using scripted Pipeline syntax

If you need to specify other forms of


=== Handling parameters

Declarative Pipeline supports parameters out-of-the-box, allowing the Pipeline
to accept user-specified parameters at runtime via the <<syntax#parameters,
parameters directive>>. Configuring parameters with scripted Pipeline is done
with the `properties` step, which can be found in the Snippet Generator.

If you configured your pipeline to accept parameters using the *Build with
Parameters* option, those parameters are accessible as members of the `params`
variable.

Assuming that a String parameter named "Greeting" has been configuring in the
`Jenkinsfile`, it  can access that parameter via `${params.Greeting}`:

[pipeline]
----
// Declarative //
pipeline {
    agent any
    parameters {
        string(name: 'Greeting', defaultValue: 'Hello', description: 'How should I greet the world?')
    }
    stages {
        stage('Example') {
            steps {
                echo "${params.Greeting} World!"
            }
        }
    }
}
// Script //
properties([parameters([string(defaultValue: 'Hello', description: 'How should I greet the world?', name: 'Greeting')])])

node {
    echo "${params.Greeting} World!"
}
----


=== Handling failure

Declarative Pipeline supports robust failure handling by default via its
<<syntax#post, post section>> which allows declaring a number of different
"post conditions" such as: `always`, `unstable`, `success`, `failure`, and
`changed`. The <<syntax, Pipeline Syntax>> section provides more detail on
how to use the various post conditions.

[pipeline]
----
// Declarative //
pipeline {
    agent any
    stages {
        stage('Test') {
            steps {
                sh 'make check'
            }
        }
    }
    post {
        always {
            junit '**/target/*.xml'
        }
        failure {
            mail to: team@example.com, subject: 'The Pipeline failed :('
        }
    }
}
// Script //
node {
    /* .. snip .. */
    stage('Test') {
        try {
            sh 'make check'
        }
        finally {
            junit '**/target/*.xml'
        }
    }
    /* .. snip .. */
}
----

[role=scripted-pipeline]
====
Scripted Pipeline however relies on Groovy's built-in `try`/`catch`/`finally` semantics
for handling failures during execution of the Pipeline.

In the <<test>> example above, the `sh` step was modified to never return a
non-zero exit code (`sh 'make check || true'`). This approach, while valid,
means the following stages need to check `currentBuild.result` to know if
there has been a test failure or not.

An alternative way of handling this, which preserves the early-exit behavior of
failures in Pipeline, while still giving `junit` the chance to capture test
reports, is to use a series of `try`/`finally` blocks:
====


=== Using multiple agents

In all the previous examples, only a single agent has been used. This means
Jenkins will allocate an executor wherever one is available, regardless of how
it is labeled or configured. Not only can this behavior be overridden, but
Pipeline allows utilizing multiple agents in the Jenkins environment from
within the _same_ `Jenkinsfile`, which can helpful for more advanced use-cases
such as  executing builds/tests across multiple platforms.

In the example below, the "Build" stage will be performed on one agent and the
built results will be reused on two subsequent agents, labelled "linux" and
"windows" respectively, during the "Test" stage.

[pipeline]
----
// Declarative //
pipeline {
    agent none
    stages {
        stage('Build') {
            agent any
            steps {
                checkout scm
                sh 'make'
                stash includes: '**/target/*.jar', name: 'app' // <1>
            }
        }
        stage('Test on Linux') {
            agent { // <2>
                label 'linux'
            }
            steps {
                unstash 'app' // <3>
                sh 'make check'
            }
            post {
                always {
                    junit '**/target/*.xml'
                }
            }
        }
        stage('Test on Windows') {
            agent {
                label 'windows'
            }
            steps {
                unstash 'app'
                bat 'make check' // <4>
            }
            post {
                always {
                    junit '**/target/*.xml'
                }
            }
        }
    }
}
// Script //
stage('Build') {
    node {
        checkout scm
        sh 'make'
        stash includes: '**/target/*.jar', name: 'app' // <1>
    }
}

stage('Test') {
    node('linux') { // <2>
        checkout scm
        try {
            unstash 'app' // <3>
            sh 'make check'
        }
        finally {
            junit '**/target/*.xml'
        }
    }
    node('windows') {
        checkout scm
        try {
            unstash 'app'
            bat 'make check' // <4>
        }
        finally {
            junit '**/target/*.xml'
        }
    }
}
----
<1> The `stash` step allows capturing files matching an inclusion pattern
(`**/target/*.jar`) for reuse within the _same_ Pipeline. Once the Pipeline has
completed its execution, stashed files are deleted from the Jenkins master.
<2> The parameter in `agent`/`node` allows for any valid Jenkins label
expression. Consult the <<syntax#, Pipeline Syntax>> section for more details.
<3> `unstash` will retrieve the named "stash" from the Jenkins master into the
Pipeline's current workspace.
<4> The `bat` script allows for executing batch scripts on Windows-based
platforms.

=== Optional step arguments

Pipeline follows the Groovy language convention of allowing parentheses to be
omitted around method arguments.

Many Pipeline steps also use the named-parameter syntax as a shorthand for
creating a Map in Groovy, which uses the syntax `[key1: value1, key2: value2]`.
Making statements like the following functionally equivalent:

[source, groovy]
----
git url: 'git://example.com/amazing-project.git', branch: 'master'
git([url: 'git://example.com/amazing-project.git', branch: 'master'])
----

For convenience, when calling steps taking only one parameter (or only one
mandatory parameter), the parameter name may be omitted, for example:

[source, groovy]
----
sh 'echo hello' /* short form  */
sh([script: 'echo hello'])  /* long form */
----


=== Advanced scripted Pipeline

Scripted Pipeline is a domain-specific language
footnoteref:[dsl, https://en.wikipedia.org/wiki/Domain-specific_language]
based on Groovy, most
link:http://groovy-lang.org/semantics.html[Groovy syntax]
can be used in scripted Pipeline without modification.


==== Parallel execution
////
NOTE: This is only under "Advanced scripted Pipeline" temporarily until some
cleaner parallel syntax is supported for declarative Pipeline. Right now
(20170201) parallel in declarative is indistinguishable from script { } based
stuff.
////

The example in the <<using-multiple-nodes,section above>> runs tests across two
different platforms in a linear series. In practice, if the `make check`
execution takes 30 minutes to complete, the "Test" stage would now take 60
minutes to complete!

Fortunately, Pipeline has built-in functionality for executing portions of
scripted Pipeline in parallel, implemented in the aptly named `parallel` step.

Refactoring the example above to use the `parallel` step:

[pipeline]
----
// Script //
stage('Build') {
    /* .. snip .. */
}

stage('Test') {
    parallel linux: {
        node('linux') {
            checkout scm
            try {
                unstash 'app'
                sh 'make check'
            }
            finally {
                junit '**/target/*.xml'
            }
        }
    },
    windows: {
        node('windows') {
            /* .. snip .. */
        }
    }
}
// Declarative not yet implemented //
----

Instead of executing the tests on the "linux" and "windows" labelled nodes in
series, they will now execute in parallel assuming the requisite capacity
exists in the Jenkins environment.
